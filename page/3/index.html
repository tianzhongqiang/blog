<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
<meta property="og:type" content="website">
<meta property="og:title" content="静水流深">
<meta property="og:url" content="http://www.thinkyixia.com/page/3/index.html">
<meta property="og:site_name" content="静水流深">
<meta property="og:description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="静水流深">
<meta name="twitter:description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">



  <link rel="alternate" href="/atom.xml" title="静水流深" type="application/atom+xml" />




  <link rel="canonical" href="http://www.thinkyixia.com/page/3/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>静水流深</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">静水流深</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Just For Fun</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/11/03/fuli/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/03/fuli/" itemprop="url">
                  复利人生
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-11-03 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-03T00:00:00+08:00">2017-11-03</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/杂说/" itemprop="url" rel="index"><span itemprop="name">杂说</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>理解时间，不慌不忙，静待时间的复利<br><img src="/images/fuli-1.png" alt=""></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/11/02/zookeeper-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/02/zookeeper-3/" itemprop="url">
                  Zookeeper的选举一：选举概述
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-11-02 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-02T00:00:00+08:00">2017-11-02</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>leader选举是zookeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。<br>我们主要从leader选举概述、算法分析和实现细节三个方面来看一下zookereper leader选举的过程。</p>
<h1 id="Zookeeper选举概述"><a href="#Zookeeper选举概述" class="headerlink" title="Zookeeper选举概述"></a>Zookeeper选举概述</h1><h2 id="服务器启动时leader的选举"><a href="#服务器启动时leader的选举" class="headerlink" title="服务器启动时leader的选举"></a>服务器启动时leader的选举</h2><p>以3台机器为例，当集群中启动一台机器的时候，是无法进行选举的，当第二台机器也启动后，此时这两台机器已经能相互通信，每台机器试图找到一个leader，于是便进入了leader选举流程。<br>1、每个Server会发出一个投票<br>  投票包含的最基本的元素包括：所推举的服务器的myid和ZXID，我们以（myid，ZXID）形式表示。初始阶段，无论是server1还是server2都会投给自己，即server1的选票（1，0），server2的选票（2，0），然后各自将这个投票发送给集群中其他所有机器。<br>2、接收来自各个服务器的投票<br>  每个服务器都会收到其他服务器的投票。收到投票后，首先判断该投票的有效性，包括检查是否是本轮投票、是否来自looking状态的服务器<br>3、处理投票<br>  在接收到来自其他服务器的投票后，针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK。<br>  PK规则如下：<br>  优先检查ZXID。ZXID比较大的服务器优先作为leader。（选择ZXID大的服务器主要是省去了新的leader产生之后，删除其他服务器上比自己ZXID大Proposal(提议)）<br>  如果ZXID相同的话，那么就比较myid。myid比较大的服务器作为leader。</p>
<p>  按以上规则，server1的投票为（1，0），收到的server2投票为（2，0）；首先对比ZXID，都是0，然后对比myid，收到的投票myid为2，大于自己的，于是就会更新自己的投票为（2，0），然后重新将投票发出去。<br>  对于server2，不需要更新自己的投票信息，只是再一次向集群中所有机器发出上一次的投票信息即可。<br>4、统计投票<br>  每次投票后，服务器都会统计所有投票，判断是否已经有过半的机器接收到相同的投票信息。<br>  对于server1和server2服务器来说，都统计出集群中已经有两台机器接受了（2，0）这个投票信息。就是说集群中已经有过半的机器收到同样的选票（2，0），即可以认为已经选出了leader。<br>  过半：过半的意思是指大于集群机器数量的一半，即大于（n／2）。这也是zookeeper推举为奇数台的原因，3台允许出问题的机器为1台，4台允许出问题的机器也是1台（需要保证存活的机器大于4/2）；而且还增加了选主的时间。<br>5、一旦确定了leader，每个服务器都会更新自己的状态，如果是follower，更新为FOLLOWING，如果是leader，那么就更改为LEADING。</p>
<h2 id="服务器运行期间leader选举"><a href="#服务器运行期间leader选举" class="headerlink" title="服务器运行期间leader选举"></a>服务器运行期间leader选举</h2><p>在zookeeper集群正常运行时，一旦选出了一个leader，那么所有服务器的集群角色一般不会发生变化。但是一旦leader所在的机器挂了，那么整个集群将暂时无法对外服务，而是进行新一轮的leader选举。<br>服务器运行期间的leader选举和启动时期的leader选举基本一样。<br>假设当前zookeeper服务器为3台，分别为server1，server2，server3，当前leader为server2。假设某一瞬间，leader挂了，此时就开始leader选举。<br>1、首先是变更状态<br>  当leader挂了，余下的非Observer服务器会将自己的服务器状态变更为LOOKING，然后开始进入Leader选举流程<br>2、每个Server会发出一个投票<br>  每个需要投票的server都生成一个投票信息（myid，ZXID），由于是运行期间，因此每个服务器上的ZXID可能不同，假设server1的ZXID为100，而server3的ZXID为99。在第一轮投票中还是都投给自己，分别产生投票（1，100）和（3，99），然后各自将这个投票发送给集群中所有机器。<br>3、接收来自各自服务器的投票<br>4、处理投票<br>  根据上面的处理规则，由于server1的ZXID为100，大于server3的ZXID，那么server3会成为leader<br>5、统计投票<br>6、改变服务器状态</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/11/02/zookeeper-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/02/zookeeper-4/" itemprop="url">
                  Zookeeper的选举二：Leader选举的算法分析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-11-02 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-02T00:00:00+08:00">2017-11-02</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="leader选举的算法分析"><a href="#leader选举的算法分析" class="headerlink" title="leader选举的算法分析"></a>leader选举的算法分析</h1><p>在zookeeper中有三种算法，分别是LeaderElection、UDP版本的FastLeaderElection和TCP版本的FastLeaderElection。不过3.4.0之后，就只保留了TCP版本的FastLeaderElection选举算法。<br>介绍FastLeaderElection选举算法算法之前，先对一些专业名次做一下解释。<br><strong>术语科普</strong></p>
<ul>
<li>SID：服务器ID<br>SID是一个数字，用来唯一标识一台zookeeper集群中的机器，每台机器不能重复，和myid的值一致。</li>
<li>ZXID：事务ID<br>ZXID是一个事务ID，用来唯一标识一次服务器状态的变更。在某一时刻，集群中每台机器的ZXID值不一定全都一样。</li>
<li>Vote：投票<br>leader选举通过投票实现。</li>
<li>Quorum：过半机器数<br>过半，就是大于集群中机器数的一半。<br>quorum = （n/2+1）</li>
</ul>
<h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><p>什么情况下会进入leader选举？</p>
<ul>
<li>服务器初始化启动的时候</li>
<li>服务器运行期间无法和leader保持连接（leader挂掉的时候）</li>
</ul>
<p>当一台机器进入leader选举流程时，当前集群处于什么状态？</p>
<ul>
<li>集群中本来就已经存在一个leader</li>
<li>集群中确实不存在leader</li>
</ul>
<h1 id="开始一次完整的投票"><a href="#开始一次完整的投票" class="headerlink" title="开始一次完整的投票"></a>开始一次完整的投票</h1><p>假设zookeeper由5台机器组成，SID为1、2、3、4、5，ZXID为9、9、9、8、8，并且此时SID为2的机器是leader。某一时刻1、2所在的机器出现故障，集群开始leader选举。</p>
<blockquote>
<p>注：如果要当前机器要选举的SID为1，ZXID为9的服务器为leader，那么它的这次投票信息可以表示为（1，9）</p>
</blockquote>
<h2 id="第一次投票"><a href="#第一次投票" class="headerlink" title="第一次投票"></a>第一次投票</h2><p>第一次投票时，由于无法检测到集群中其他机器的状态信息，所以每台机器都会投自己。于是SID为3、4、5的机器，投票情况为（3，9）、（4、8）、（5，9）</p>
<h2 id="变更投票"><a href="#变更投票" class="headerlink" title="变更投票"></a>变更投票</h2><p>每台机器发出投票后，也会收到投票，根据一定规则，来处理其他机器的投票，并以此来决定是否需要更改自己的投票。<br><strong>术语科普</strong></p>
<ul>
<li>vote_sid：接收到的投票中所推举leader服务器的SID</li>
<li>vote_zxid：接收到的投票中所推举leader服务器的ZXID</li>
<li>self_sid：当前服务器自己的SID</li>
<li>self_zxid：当前服务器自己的ZXID</li>
</ul>
<p>每次对于收到的投票结果都是一个对（vote_sid,vote_zxid）和（self_sid，self_zxid）对比的过程</p>
<p><strong>对比规则</strong></p>
<ul>
<li>如果vote_zxid大于self_zxid，就认可当前收到的投票，（将自己的投票改投收到的SID），并再次将该投票发送出去</li>
<li>如果vote_zxid小于self_zxid，那么就坚持自己的投票，不做任何变更</li>
<li>如果vote_zxid等于self_zxid，那么就对比两者SID。vote_sid大于self_sid，就认可当前收到的投票（将自己的投票改投收到的SID），并再次将该投票发送出去</li>
<li>vote_sid小于self_sid，坚持自己的投票，不做任何变更</li>
</ul>
<p><strong>对应机器投票变更情况</strong></p>
<ul>
<li>对于server3（3，9）来说，收到（4，8）和（5，8）两个投票，对比后，由于自己的ZXID大于两者，因此不需要变更，最终还是坚持投票（3，9）</li>
<li>对于server4（4，8）来说，收到（3，9）和（5，8）两个投票，由于（3，9）这个投票的ZXID大于自己的ZXID，因此需要将投票变更为（3，9），然后继续将投票发送给另外两台机器。</li>
<li>对于server5 （5，8）来说，收到（3，9）和（4，8）两个投票，对比后，（3，9）这个投票的ZXID大于自己，一次需要变更投票为（3，9），然后继续将这个投票发送给另外两台机器</li>
</ul>
<h2 id="确定leader"><a href="#确定leader" class="headerlink" title="确定leader"></a>确定leader</h2><p>经过第二轮投票后，集群中的每台机器会再次收到其他机器的投票，然后开始统计投票。当一台机器收到来超过半数相同的投票，那么这个投票对应的SID机器即为leader。<br>这里，server3、server4、server5的投票都是（3，9），所以server3为leader</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>简单来说，就是那台机器的数据越新，就越有可能成为leader，数据越新，ZXID也就越大，也就能够保证数据的恢复。当然，如果集群中有几台服务器有相同的ZXID，那么SID较大的服务器会成为leader。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/11/02/zookeeper-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/02/zookeeper-5/" itemprop="url">
                  Zookeeper的选举三：Leader选举的实现细节
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-11-02 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-02T00:00:00+08:00">2017-11-02</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Leader选举的实现细节"><a href="#Leader选举的实现细节" class="headerlink" title="Leader选举的实现细节"></a>Leader选举的实现细节</h1><p><strong>服务器的状态</strong></p>
<ul>
<li>LOOKING：寻找Leader状态，当服务器处于该状态时，会认为当前集群中没有leader，因此需要进入leader选举流程</li>
<li>FOLLOWING：跟随者状态，当前服务器是Follower</li>
<li>LEADING：领导者状态，当前服务器为Leader</li>
<li>OBSERVING：观察者状态，当前服务器角色是Observer</li>
</ul>
<p><strong>Vote数据结构说明</strong></p>
<ul>
<li>id：被推举的leader的SID值</li>
<li>zxid：被推举的leader的事务ID</li>
<li>electionEpoch：用来判断多个投票是否在统一轮选举周期中。该值在服务端是一个自增序列，每次进入新一轮投票，都会对该值进行加1操作</li>
<li>peerEpoch：被推举leader的epoch</li>
<li>state：当前服务器的状态</li>
</ul>
<h2 id="QuorumCnxManager：网络IO"><a href="#QuorumCnxManager：网络IO" class="headerlink" title="QuorumCnxManager：网络IO"></a>QuorumCnxManager：网络IO</h2><p>每台服务器启动的时候，都会启动一个QuorumCnxManager，负责各台服务器之间的底层leader选举工程中的网络通信。</p>
<ol>
<li>消息队列<br>QuorumCnxManager这个类内部维护了一系列的队列，用于保存接收到的、待发送的消息。</li>
</ol>
<ul>
<li>recvQueue：消息接收队列，用于存放那些从其他服务器接收到的消息</li>
<li>queueSendMap：消息发送队列，用于保存那些待发送的消息；它是一个Map，按SID进行分组，分别为集群中每台机器分配一个单独队列，从而保证各台机器之间的消息发送互不影响</li>
<li>senderWorkerMap：发送器集合。每个SendWorker消息发送器，都对应一台远程Zookeeper服务器，负责消息的发送。在senderWorkerMap中，也按照SID进行了分组</li>
<li>lastMessageSent：最近发送过的消息。这个集合中，为每个SID保留最近发送过的一个消息。</li>
</ul>
<ol>
<li>建立连接<br>QuorumCnxManager在启动的时候，会创建一个ServerSocket来监听leader选举的通信端口（默认3888）。开启端口监听后，Zookeeper会不断的接收到来自其他服务器的“创建连接”请求，在接收到其他服务器的TCP连接请求时，会交给receiveConnection函数来处理。<br>为了避免两台机器之间重复创建TCP连接，zookeeper设计了一种建立TCP连接的规则：<blockquote>
<p>只允许SID大的服务器主动和其他服务器建立连接，否则断开连接。<br>服务器接收到请求后，如果发现当前服务器的SID比接收到的大，就断开连接，然后自己主动去和远程服务器建立连接</p>
</blockquote>
</li>
</ol>
<p>一旦建立连接，就会根据远程服务器的SID创建相应的消息发送器SendWorker和消息接收器RecvWorker，并启动他们。</p>
<ol>
<li><p>消息接收和发送<br>在完成选票的初始化之后，服务器就会发起第一次投票。Zookeeper会将刚刚初始化好的选票放入sendqueue队列中，由发送器WorkerSender负责发送出去</p>
</li>
<li><p>接收外部投票<br>每台服务器会不断的从recvqueue队列中获取外部投票。如果服务器发现无法获取到任何的外部投票，那么就会立即确认自己是否和集群中其他服务器保持着有效连接。如果发现没有建立连接，就马上建立连接；如果已经建立了连接，那么就再次发送自己当前的内部投票。</p>
</li>
<li><p>判断选举轮次<br>在处理外部投票的时候，会根据选举轮次来进行不同处理。</p>
</li>
</ol>
<ul>
<li>外部选票的选举轮次大于内部选票<br>当服务器发现自己的选举轮次已经落后与该外部投票对应服务器的选举轮次，那么就会立即更新自己的选举轮次（logicalclock），并且清空所有已经收到的投票，然后使用初始化的投票来进行PK以确定是否变更内部投票，最终再将内部投票发送出去</li>
<li>外部投票的选举轮次小于内部投票<br>如果接收到的选票的选举轮次落后于自己的，那么就直接忽略该外部投票</li>
<li>外部投票的选举轮次和内部投票一样<br>直接根据对比逻辑进行PK</li>
</ul>
<p>只有在统一选举轮次的投票才是有效的</p>
<ol>
<li>选票PK</li>
</ol>
<ul>
<li>如果外部投票中被推举的leader服务器的选举轮次大于内部投票，那么就进行投票变更。小于就忽略</li>
<li>如果投票轮次一样，就比较两者的SID。如果外部投票的ZXID大于内部投票，就进行投票变更。小于就不变</li>
<li>如果两者ZXID一样，那么就比对两者的SID。如果外部投票的SID大于内部投票，那么就需要进行投票变更。小于就忽略</li>
</ul>
<ol>
<li><p>变更投票<br>如果PK失败，那么就使用外部投票信息覆盖内部投票信息。变更完成后，再次将这个变更后的内部投票发送出去</p>
</li>
<li><p>选票归档<br>无论是否进行了投票更新，都会将刚刚收到的那部分外部投票放入“选票集合” recvset中进行归档。<br>recvset用于记录当前服务器在本轮次的leader选举中收到的所有外部投票，按照对应的SID来区分</p>
</li>
<li><p>完成了选票归档之后，就可以开始统计投票了。如果确定已经有过半的服务器认可了该内部投票，就终止投票。</p>
<blockquote>
<p>注：服务器发现有过半的服务器认可当前投票时，并不会立即更新服务器状态，而是等待一段时间（默认200毫秒）来确定是否有新的更优的投票</p>
</blockquote>
</li>
<li><p>更新服务器状态<br>终止投票后，就更新服务器状态。leader服务器更新状态为LEADING，follower服务器更新状态FOLLOWEING</p>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/27/zookeeper-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/27/zookeeper-2/" itemprop="url">
                  Zookeeper核心算法ZAB
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-27 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-27T00:00:00+08:00">2017-10-27</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ZAB协议概述"><a href="#ZAB协议概述" class="headerlink" title="ZAB协议概述"></a>ZAB协议概述</h1><p>ZAB协议是为分布式协调服务Zookeeper专门设计的一种支持崩溃恢复的原子广播协议，Zookeeper中主要依赖ZAB协议来实现分布式数据一致性。Zookeeper使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事务Proposal的形式广播到所有的副本进程上去。<br>ZAB协议的核心是定义了对于那些会改变Zookeeper服务器数据状态的事务请求的处理方式，即：<br>所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为leader服务器，而余下的服务器称为follower服务器。leader服务器负责将一个客户端事务请求转换成一个事务Proposal（提议），并将该Proposal分发给集群中所有的follower服务器。之后leader服务器需要等待所有follower服务器的反馈，一旦超过半数的follower服务器进行了正确的反馈后，那么leader就会再次向所有的follower服务器分发commit消息，要求其将前一个Proposal进行提交。</p>
<h1 id="ZAB协议具体内容"><a href="#ZAB协议具体内容" class="headerlink" title="ZAB协议具体内容"></a>ZAB协议具体内容</h1><p>ZAB协议的具体内容，包括两种模式：崩溃模式和消息广播；<br>当整个服务框架在启动的过程中或是当leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的leader服务器。当选举产生了新的leader服务器，同时集群中已经有过半的机器与该leader服务器完成了状态同步（数据同步）之后，ZAB协议就会退出恢复模式。并进入消息广播模式。其中，所谓的状态同步就是指数据同步。<br>若有新的服务器加入集群时，如果此时集群中存在leader在负责消息广播，那么新加入的机器会进入数据恢复模式：找到leader，与leader进行数据同步，然后一起参与到消息广播流程中。<br>当leader服务器出现崩溃或集群中已经不存在过半的服务器与leader服务器保持正常通信时，所有机器首先会使用崩溃恢复协议来使彼此达到一个一致的状态，此时从消息广播模式进入崩溃恢复模式。</p>
<p>上面介绍了一下两种模式切换时的情况，接下来详细介绍一下这两种模式：</p>
<h2 id="消息广播"><a href="#消息广播" class="headerlink" title="消息广播"></a>消息广播</h2><p>客户端的事务请求，leader服务器为其生成对应的事务Proposal，并将其发送给其余的所有机器，然后再分别收集各自的选票，最后进行事务提交。</p>
<p><storage>消息广播的过程</storage></p>
<ol>
<li>leader服务器为每个事务请求生成对应的Proposal来进行广播</li>
<li>在广播事务Proposal之前，leader服务器会首先为这个事务Proposal分配一个全局单调递增的唯一ID，我们称为事务ID（即ZXID）。每一个事务Proposal会按照ZXID的先后顺序进行排序和处理</li>
<li>在消息广播过程中，leader服务器为每一个follower服务器各自分配一个单独的队列，将需要广播的事务Proposal依次放入这些队列中，并根据FIFO策略进行消息发送</li>
<li>每一个follower服务器在接收到这个事务Proposal之后，首先将其以事务日志的形式写入本地磁盘。</li>
<li>写入成功之后反馈给leader服务器一个ACK响应。</li>
<li>当leader服务器接收到超过半数follower（根据配置文件中配置的机器）的Ack响应后，就会广播一个Commit消息给所有的follower服务器以通知其进行事务提交，同时leader自己也会完成对事务的提交</li>
<li>每一个follower服务器在接收到Commit消息后，也会完成对事务的提交</li>
</ol>
<h2 id="崩溃恢复"><a href="#崩溃恢复" class="headerlink" title="崩溃恢复"></a>崩溃恢复</h2><h3 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h3><p>当leader服务器出现崩溃，或者由于网络原因导致leader服务器失去了与过半follower的联系，就会进入崩溃恢复模式；<br>在ZAB协议中，整个恢复过程结束后需要选举新的leader服务器。</p>
<p><storage>崩溃恢复中可能出现数据不一致的隐患：<storage></storage></storage></p>
<ol>
<li>ZAB协议需要确保那些已经在leader服务器上提交的事务最终被所有服务器都提交<br>假设一个Proposel（事务）在leader服务器上被提交了，并且已经得到过半follower服务器的Ack反馈，但是在它将commit消息发送给所有follower机器之前，leader服务器挂了。</li>
<li>ZAB协议需要确保丢弃那些只在leader服务器上被提出的事务<br>假设当leader服务器提出一个事务之后就崩溃退出来，从而导致集群中其他服务器都没有收到这个事务，于是当该服务器恢复过来再次加入集群的时候，ZAB协议需要确认丢弃这个事务。</li>
</ol>
<p>由于上面的两种情况，就需要ZAB协议设计这样一个leader选举算法：能够确保提交已经被leader提交的事务Proposal，同时丢弃已经被跳过的事务Proposal。<br>针对这个要求：<br>如果让leader选举算法能够保证新选举出来的leader服务器拥有集群中所有机器最高编号（即ZXID最大）的事务Proposal，那么就可以保证这个新选举出来的leader一定具有所有已经提交的提案。更重要的是，如果让具有最高编号事务Proposal的机器来成为leader，就可以省去leader服务器检查Proposal的提交和丢弃工作的操作了。</p>
<h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><p>在leader选举之后，正式开始工作（即接收客户端的事务请求，然后提出新的提案）之前，leader服务器会首先确认事务日志中的所有Proposal是否都已经被集群中过半的机器提交了（即是否完成数据同步）。</p>
<p><storage>ZAB协议的数据同步过程<storage></storage></storage></p>
<ol>
<li>leader服务器会为每一个follower服务器准备一个队列，并将那些没有被各follower服务器同步的事务以Proposal消息的形式逐个发送给follower服务器，并在每一个Proposal消息后紧接着再发送一个Commit消息，以表示该事务已经被提交</li>
<li>等到follower服务器将所有其尚未同步的事务Proposal都从leader服务器上同步过来并成功应用到本地数据库中之后，，leader服务器就会将该follower服务器加入到真正可用的follower列表中，并开始之后的其他流程</li>
</ol>
<p>上面是正常的数据同步逻辑，那么ZAB协议是如何处理那些需要被丢弃的事务Proposal呢？</p>
<p><storage>主要是基于ZAB协议的事务编号ZXID设计策略：</storage></p>
<ul>
<li>ZXID是一个64位的数字；</li>
<li>其中低32位可以看作一个简单的单调递增的计数器，针对客户端的每一个事务请求，leader服务器在产生一个新的事务Proposal的时候，都会对该计数器进行加1操作；</li>
<li>高32位则代表了leader周期epoch的编号，每当选举产生一个新的leader服务器，就会从这个leader服务器上取出其本地日志中最大的事务Proposal的ZXID，并从该ZXID中解析出对应的epoch值，然后再对其进行加1操作，之后就会以此编号作为新的epoch，并将低32位置为0，开始生成新的ZXID。</li>
<li>ZAB协议中就是通过epoch编号来区分leader周期变化的，这个策略能够有效避免不同的leader服务器错误的使用相同的ZXID编号提出不一样的事务Proposal的异常情况,这对于识别在leader崩溃恢复前后生成的Proposal非常有帮助，大大简化和提升了数据恢复流程</li>
<li>基于这样的策略，当一个包含了上一个leader周期中尚未提交过的事务Proposal的服务器启动后，其肯定无法成为leader。<br>原因很简单，因为当前集群中一定包含一个Quorum集合，该集合中的机器一定包含了更高epoch的事务Proposal，因此这台机器的事务Proposal肯定不是最高，也就无法成为leader。</li>
<li>当这台机器加入到集群中，以follower角色连接上leader服务器以后，leader服务器会根据自己服务器上最后被提交的Proposal来和follower服务器的Proposal进行比对，比对的结果当然是leader会要求follower进行回退操作（回退到一个确实已经被集群中过半机器提交的最新的事务Proposal）</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/25/kafka-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/25/kafka-2/" itemprop="url">
                  Kafka史上最详细原理总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-25 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-25T00:00:00+08:00">2017-10-25</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><storage><br>文章转自：<a href="http://www.itkeyword.com/doc/3033455819328241799/kafka-apache-scala" target="_blank" rel="noopener">http://www.itkeyword.com/doc/3033455819328241799/kafka-apache-scala</a><br></storage></p>
<h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p>Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。</p>
<h2 id="Kafka的特性"><a href="#Kafka的特性" class="headerlink" title="Kafka的特性:"></a>Kafka的特性:</h2><ol>
<li>高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。</li>
<li>可扩展性：kafka集群支持热扩展</li>
<li>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</li>
<li>容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）</li>
<li>高并发：支持数千个客户端同时读写</li>
</ol>
<h2 id="Kafka的使用场景："><a href="#Kafka的使用场景：" class="headerlink" title="Kafka的使用场景："></a>Kafka的使用场景：</h2><ol>
<li>日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。</li>
<li>消息系统：解耦和生产者和消费者、缓存消息等。</li>
<li>用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。</li>
<li>运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。</li>
<li>流式处理：比如spark streaming和storm</li>
<li>事件源</li>
</ol>
<h2 id="Kakfa的设计思想"><a href="#Kakfa的设计思想" class="headerlink" title="Kakfa的设计思想"></a>Kakfa的设计思想</h2><ul>
<li>Kakfa Broker Leader的选举：Kakfa Broker集群受Zookeeper管理。所有的Kafka Broker节点一起去Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower。（这个过程叫Controller在ZooKeeper注册Watch）。这个Controller会监听其他的Kafka Broker的所有信息，如果这个kafka broker controller宕机了，在zookeeper上面的那个临时节点就会消失，此时所有的kafka broker又会一起去Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower。例如：一旦有一个broker宕机了，这个kafka broker controller会读取该宕机broker上所有的partition在zookeeper上的状态，并选取ISR列表中的一个replica作为partition leader（如果ISR列表中的replica全挂，选一个幸存的replica作为leader; 如果该partition的所有的replica都宕机了，则将新的leader设置为-1，等待恢复，等待ISR中的任一个Replica“活”过来，并且选它作为Leader；或选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader），这个broker宕机的事情，kafka controller也会通知zookeeper，zookeeper就会通知其他的kafka broker。<font color="#FF0000"><br>这里曾经发生过一个bug，TalkingData使用Kafka0.8.1的时候，kafka controller在Zookeeper上注册成功后，它和Zookeeper通信的timeout时间是6s，也就是如果kafka controller如果有6s中没有和Zookeeper做心跳，那么Zookeeper就认为这个kafka controller已经死了，就会在Zookeeper上把这个临时节点删掉，那么其他Kafka就会认为controller已经没了，就会再次抢着注册临时节点，注册成功的那个kafka broker成为controller，然后，之前的那个kafka controller就需要各种shut down去关闭各种节点和事件的监听。但是当kafka的读写流量都非常巨大的时候，TalkingData的一个bug是，由于网络等原因，kafka controller和Zookeeper有6s中没有通信，于是重新选举出了一个新的kafka controller，但是原来的controller在shut down的时候总是不成功，这个时候producer进来的message由于Kafka集群中存在两个kafka controller而无法落地。导致数据淤积。</font><br><font color="#FF0000">这里曾经还有一个bug，TalkingData使用Kafka0.8.1的时候，当ack=0的时候，表示producer发送出去message，只要对应的kafka broker topic partition leader接收到的这条message，producer就返回成功，不管partition leader 是否真的成功把message真正存到kafka。当ack=1的时候，表示producer发送出去message，同步的把message存到对应topic的partition的leader上，然后producer就返回成功，partition leader异步的把message同步到其他partition replica上。当ack=all或-1，表示producer发送出去message，同步的把message存到对应topic的partition的leader和对应的replica上之后，才返回成功。但是如果某个kafka controller 切换的时候，会导致partition leader的切换（老的 kafka controller上面的partition leader会选举到其他的kafka broker上）,但是这样就会导致丢数据。</font></li>
<li>Consumergroup：各个consumer（consumer 线程）可以组成一个组（Consumer group ），partition中的每个message只能被组（Consumer group ）中的一个consumer（consumer 线程）消费，如果一个message可以被多个consumer（consumer 线程）消费的话，那么这些consumer必须在不同的组。Kafka不支持一个partition中的message由两个或两个以上的同一个consumer group下的consumer thread来处理，除非再启动一个新的consumer group。所以如果想同时对一个topic做消费的话，启动多个consumer group就可以了，但是要注意的是，这里的多个consumer的消费都必须是顺序读取partition里面的message，新启动的consumer默认从partition队列最头端最新的地方开始阻塞的读message。它不能像AMQ那样可以多个BET作为consumer去互斥的（for update悲观锁）并发处理message，这是因为多个BET去消费一个Queue中的数据的时候，由于要保证不能多个线程拿同一条message，所以就需要行级别悲观所（for update）,这就导致了consume的性能下降，吞吐量不够。而kafka为了保证吞吐量，只允许同一个consumer group下的一个consumer线程去访问一个partition。如果觉得效率不高的时候，可以加partition的数量来横向扩展，那么再加新的consumer thread去消费。如果想多个不同的业务都需要这个topic的数据，起多个consumer group就好了，大家都是顺序的读取message，offsite的值互不影响。这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。<br> 当启动一个consumer group去消费一个topic的时候，无论topic里面有多个少个partition，无论我们consumer group里面配置了多少个consumer thread，这个consumer group下面的所有consumer thread一定会消费全部的partition；即便这个consumer group下只有一个consumer thread，那么这个consumer thread也会去消费所有的partition。因此，最优的设计就是，consumer group下的consumer thread的数量等于partition数量，这样效率是最高的。<br> 同一partition的一条message只能被同一个Consumer Group内的一个Consumer消费。不能够一个consumer group的多个consumer同时消费一个partition。<br> 一个consumer group下，无论有多少个consumer，这个consumer group一定回去把这个topic下所有的partition都消费了。当consumer group里面的consumer数量小于这个topic下的partition数量的时候，如下图groupA,groupB，就会出现一个conusmer thread消费多个partition的情况，总之是这个topic下的partition都会被消费。如果consumer group里面的consumer数量等于这个topic下的partition数量的时候，如下图groupC，此时效率是最高的，每个partition都有一个consumer thread去消费。当consumer group里面的consumer数量大于这个topic下的partition数量的时候，如下图GroupD，就会有一个consumer thread空闲。因此，我们在设定consumer group的时候，只需要指明里面有几个consumer数量即可，无需指定对应的消费partition序号，consumer会自动进行rebalance。<br> 多个Consumer Group下的consumer可以消费同一条message，但是这种消费也是以o（1）的方式顺序的读取message去消费,，所以一定会重复消费这批message的，不能向AMQ那样多个BET作为consumer消费（对message加锁，消费的时候不能重复消费message）</li>
<li>Consumer Rebalance的触发条件：（1）Consumer增加或删除会触发 Consumer Group的Rebalance（2）Broker的增加或者减少都会触发 Consumer Rebalance</li>
<li><p>Consumer： Consumer处理partition里面的message的时候是o（1）顺序读取的。所以必须维护着上一次读到哪里的offsite信息。high level API,offset存于Zookeeper中，low level API的offset由自己维护。一般来说都是使用high level api的。Consumer的delivery gurarantee，默认是读完message先commmit再处理message，autocommit默认是true，这时候先commit就会更新offsite+1，一旦处理失败，offsite已经+1，这个时候就会丢message；也可以配置成读完消息处理再commit，这种情况下consumer端的响应就会比较慢的，需要等处理完才行。<br>一般情况下，一定是一个consumer group处理一个topic的message。Best Practice是这个consumer group里面consumer的数量等于topic里面partition的数量，这样效率是最高的，一个consumer thread处理一个partition。如果这个consumer group里面consumer的数量小于topic里面partition的数量，就会有consumer thread同时处理多个partition（这个是kafka自动的机制，我们不用指定），但是总之这个topic里面的所有partition都会被处理到的。。如果这个consumer group里面consumer的数量大于topic里面partition的数量，多出的consumer thread就会闲着啥也不干，剩下的是一个consumer thread处理一个partition，这就造成了资源的浪费，因为一个partition不可能被两个consumer thread去处理。所以我们线上的分布式多个service服务，每个service里面的kafka consumer数量都小于对应的topic的partition数量，但是所有服务的consumer数量只和等于partition的数量，这是因为分布式service服务的所有consumer都来自一个consumer group，如果来自不同的consumer group就会处理重复的message了（同一个consumer group下的consumer不能处理同一个partition，不同的consumer group可以处理同一个topic，那么都是顺序处理message，一定会处理重复的。一般这种情况都是两个不同的业务逻辑，才会启动两个consumer group来处理一个topic）。</p>
<p>如果producer的流量增大，当前的topic的parition数量=consumer数量，这时候的应对方式就是很想扩展：增加topic下的partition，同时增加这个consumer group下的consumer。<br><img src="/images/kafka-2-1.png" alt=""></p>
</li>
<li><p>Delivery Mode : Kafka producer 发送message不用维护message的offsite信息，因为这个时候，offsite就相当于一个自增id，producer就尽管发送message就好了。而且Kafka与AMQ不同，AMQ大都用在处理业务逻辑上，而Kafka大都是日志，所以Kafka的producer一般都是大批量的batch发送message，向这个topic一次性发送一大批message，load balance到一个partition上，一起插进去，offsite作为自增id自己增加就好。但是Consumer端是需要维护这个partition当前消费到哪个message的offsite信息的，这个offsite信息，high level api是维护在Zookeeper上，low level api是自己的程序维护。（Kafka管理界面上只能显示high level api的consumer部分，因为low level api的partition offsite信息是程序自己维护，kafka是不知道的，无法在管理界面上展示 ）当使用high level api的时候，先拿message处理，再定时自动commit offsite+1（也可以改成手动）, 并且kakfa处理message是没有锁操作的。因此如果处理message失败，此时还没有commit offsite+1，当consumer thread重启后会重复消费这个message。但是作为高吞吐量高并发的实时处理系统，at least once的情况下，至少一次会被处理到，是可以容忍的。如果无法容忍，就得使用low level api来自己程序维护这个offsite信息，那么想什么时候commit offsite+1就自己搞定了。</p>
</li>
<li><p>Topic &amp; Partition：Topic相当于传统消息系统MQ中的一个队列queue，producer端发送的message必须指定是发送到哪个topic，但是不需要指定topic下的哪个partition，因为kafka会把收到的message进行load balance，均匀的分布在这个topic下的不同的partition上（ hash(message) % [broker数量]  ）。物理上存储上，这个topic会分成一个或多个partition，每个partiton相当于是一个子queue。在物理结构上，每个partition对应一个物理的目录（文件夹），文件夹命名是[topicname]<em>[partition]</em>[序号]，一个topic可以有无数多的partition，根据业务需求和数据量来设置。在kafka配置文件中可随时更高num.partitions参数来配置更改topic的partition数量，在创建Topic时通过参数指定parittion数量。Topic创建之后通过Kafka提供的工具也可以修改partiton数量。<br> 一般来说，（1）一个Topic的Partition数量大于等于Broker的数量，可以提高吞吐率。（2）同一个Partition的Replica尽量分散到不同的机器，高可用。<br>当add a new partition的时候，partition里面的message不会重新进行分配，原来的partition里面的message数据不会变，新加的这个partition刚开始是空的，随后进入这个topic的message就会重新参与所有partition的load balance</p>
</li>
<li>Partition Replica：每个partition可以在其他的kafka broker节点上存副本，以便某个kafka broker节点宕机不会影响这个kafka集群。存replica副本的方式是按照kafka broker的顺序存。例如有5个kafka broker节点，某个topic有3个partition，每个partition存2个副本，那么partition1存broker1,broker2，partition2存broker2,broker3。。。以此类推（replica副本数目不能大于kafka broker节点的数目，否则报错。这里的replica数其实就是partition的副本总数，其中包括一个leader，其他的就是copy副本）。这样如果某个broker宕机，其实整个kafka内数据依然是完整的。但是，replica副本数越高，系统虽然越稳定，但是回来带资源和性能上的下降；replica副本少的话，也会造成系统丢数据的风险。<br>（1）怎样传送消息：producer先把message发送到partition leader，再由leader发送给其他partition follower。（如果让producer发送给每个replica那就太慢了）<br>（2）在向Producer发送ACK前需要保证有多少个Replica已经收到该消息：根据ack配的个数而定<br>（3）怎样处理某个Replica不工作的情况：如果这个部工作的partition replica不在ack列表中，就是producer在发送消息到partition leader上，partition leader向partition follower发送message没有响应而已，这个不会影响整个系统，也不会有什么问题。如果这个不工作的partition replica在ack列表中的话，producer发送的message的时候会等待这个不工作的partition replca写message成功，但是会等到time out，然后返回失败因为某个ack列表中的partition replica没有响应，此时kafka会自动的把这个部工作的partition replica从ack列表中移除，以后的producer发送message的时候就不会有这个ack列表下的这个部工作的partition replica了。<br>（4）怎样处理Failed Replica恢复回来的情况：如果这个partition replica之前不在ack列表中，那么启动后重新受Zookeeper管理即可，之后producer发送message的时候，partition leader会继续发送message到这个partition follower上。如果这个partition replica之前在ack列表中，此时重启后，需要把这个partition replica再手动加到ack列表中。（ack列表是手动添加的，出现某个部工作的partition replica的时候自动从ack列表中移除的）</li>
<li>Partition leader与follower：partition也有leader和follower之分。leader是主partition，producer写kafka的时候先写partition leader，再由partition leader push给其他的partition follower。partition leader与follower的信息受Zookeeper控制，一旦partition leader所在的broker节点宕机，zookeeper会冲其他的broker的partition follower上选择follower变为parition leader。</li>
<li><p>Topic分配partition和partition replica的算法：（1）将Broker（size=n）和待分配的Partition排序。（2）将第i个Partition分配到第（i%n）个Broker上。（3）将第i个Partition的第j个Replica分配到第（(i + j) % n）个Broker上</p>
</li>
<li><p>消息投递可靠性<br>一个消息如何算投递成功，Kafka提供了三种模式：</p>
</li>
<li>第一种是啥都不管，发送出去就当作成功，这种情况当然不能保证消息成功投递到broker；</li>
<li>第二种是Master-Slave模型，只有当Master和所有Slave都接收到消息时，才算投递成功，这种模型提供了最高的投递可靠性，但是损伤了性能；</li>
<li><p>第三种模型，即只要Master确认收到消息就算投递成功；实际使用时，根据应用特性选择，绝大多数情况下都会中和可靠性和性能选择第三种模型<br>消息在broker上的可靠性，因为消息会持久化到磁盘上，所以如果正常stop一个broker，其上的数据不会丢失；但是如果不正常stop，可能会使存在页面缓存来不及写入磁盘的消息丢失，这可以通过配置flush页面缓存的周期、阈值缓解，但是同样会频繁的写磁盘会影响性能，又是一个选择题，根据实际情况配置。<br>消息消费的可靠性，Kafka提供的是“At least once”模型，因为消息的读取进度由offset提供，offset可以由消费者自己维护也可以维护在zookeeper里，但是当消息消费后consumer挂掉，offset没有即时写回，就有可能发生重复读的情况，这种情况同样可以通过调整commit offset周期、阈值缓解，甚至消费者自己把消费和commit offset做成一个事务解决，但是如果你的应用不在乎重复消费，那就干脆不要解决，以换取最大的性能。</p>
</li>
<li><p>Partition ack：当ack=1，表示producer写partition leader成功后，broker就返回成功，无论其他的partition follower是否写成功。当ack=2，表示producer写partition leader和其他一个follower成功的时候，broker就返回成功，无论其他的partition follower是否写成功。当ack=-1[parition的数量]的时候，表示只有producer全部写成功的时候，才算成功，kafka broker才返回成功信息。这里需要注意的是，如果ack=1的时候，一旦有个broker宕机导致partition的follower和leader切换，会导致丢数据。<br><img src="/images/kafka-2-2.png" alt=""></p>
</li>
<li><p>message状态：在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置），这就意味着如果consumer处理不好的话，broker上的一个消息可能会被消费多次。</p>
</li>
<li>message持久化：Kafka中会把消息持久化到本地文件系统中，并且保持o(1)极高的效率。我们众所周知IO读取是非常耗资源的性能也是最慢的，这就是为了数据库的瓶颈经常在IO上，需要换SSD硬盘的原因。但是Kafka作为吞吐量极高的MQ，却可以非常高效的message持久化到文件。这是因为Kafka是顺序写入o（1）的时间复杂度，速度非常快。也是高吞吐量的原因。由于message的写入持久化是顺序写入的，因此message在被消费的时候也是按顺序被消费的，保证partition的message是顺序消费的。一般的机器,单机每秒100k条数据。</li>
<li>message有效期：Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。</li>
<li>Produer : Producer向Topic发送message，不需要指定partition，直接发送就好了。kafka通过partition ack来控制是否发送成功并把信息返回给producer，producer可以有任意多的thread，这些kafka服务器端是不care的。Producer端的delivery guarantee默认是At least once的。也可以设置Producer异步发送实现At most once。Producer可以用主键幂等性实现Exactly once</li>
<li>Kafka高吞吐量： Kafka的高吞吐量体现在读写上，分布式并发的读和写都非常快，写的性能体现在以o(1)的时间复杂度进行顺序写入。读的性能体现在以o(1)的时间复杂度进行顺序读取， 对topic进行partition分区，consume group中的consume线程可以以很高能性能进行顺序读。</li>
<li>Kafka delivery guarantee(message传送保证)：（1）At most once消息可能会丢，绝对不会重复传输；（2）At least once 消息绝对不会丢，但是可能会重复传输；（3）Exactly once每条信息肯定会被传输一次且仅传输一次，这是用户想要的。</li>
<li>批量发送：Kafka支持以消息集合为单位进行批量发送，以提高push效率。</li>
<li>push-and-pull : Kafka中的Producer和consumer采用的是push-and-pull模式，即Producer只管向broker push消息，consumer只管从broker pull消息，两者对消息的生产和消费是异步的。</li>
<li>Kafka集群中broker之间的关系：不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。</li>
<li>负载均衡方面： Kafka提供了一个 metadata API来管理broker之间的负载（对Kafka0.8.x而言，对于0.7.x主要靠zookeeper来实现负载均衡）。</li>
<li>同步异步：Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。</li>
<li>分区机制partition：Kafka的broker端支持消息分区partition，Producer可以决定把消息发到哪个partition，在一个partition 中message的顺序就是Producer发送消息的顺序，一个topic中可以有多个partition，具体partition的数量是可配置的。partition的概念使得kafka作为MQ可以横向扩展，吞吐量巨大。partition可以设置replica副本，replica副本存在不同的kafka broker节点上，第一个partition是leader,其他的是follower，message先写到partition leader上，再由partition leader push到parition follower上。所以说kafka可以水平扩展，也就是扩展partition。</li>
<li>离线数据装载：Kafka由于对可拓展的数据持久化的支持，它也非常适合向Hadoop或者数据仓库中进行数据装载。</li>
<li>实时数据与离线数据：kafka既支持离线数据也支持实时数据，因为kafka的message持久化到文件，并可以设置有效期，因此可以把kafka作为一个高效的存储来使用，可以作为离线数据供后面的分析。当然作为分布式实时消息系统，大多数情况下还是用于实时的数据处理的，但是当cosumer消费能力下降的时候可以通过message的持久化在淤积数据在kafka。</li>
<li>插件支持：现在不少活跃的社区已经开发出不少插件来拓展Kafka的功能，如用来配合Storm、Hadoop、flume相关的插件。</li>
<li>解耦:  相当于一个MQ，使得Producer和Consumer之间异步的操作，系统之间解耦</li>
<li>冗余:  replica有多个副本，保证一个broker node宕机后不会影响整个服务</li>
<li>扩展性:  broker节点可以水平扩展，partition也可以水平增加，partition replica也可以水平增加</li>
<li>峰值:  在访问量剧增的情况下，kafka水平扩展, 应用仍然需要继续发挥作用</li>
<li>可恢复性:  系统的一部分组件失效时，由于有partition的replica副本，不会影响到整个系统。</li>
<li>顺序保证性：由于kafka的producer的写message与consumer去读message都是顺序的读写，保证了高效的性能。</li>
<li>缓冲：由于producer那面可能业务很简单，而后端consumer业务会很复杂并有数据库的操作，因此肯定是producer会比consumer处理速度快，如果没有kafka，producer直接调用consumer，那么就会造成整个系统的处理速度慢，加一层kafka作为MQ，可以起到缓冲的作用。</li>
<li>异步通信：作为MQ，Producer与Consumer异步通信</li>
</ul>
<h1 id="Kafka文件存储机制"><a href="#Kafka文件存储机制" class="headerlink" title="Kafka文件存储机制"></a>Kafka文件存储机制</h1><h2 id="Kafka部分名词解释如下："><a href="#Kafka部分名词解释如下：" class="headerlink" title="Kafka部分名词解释如下："></a>Kafka部分名词解释如下：</h2><ul>
<li>Kafka中发布订阅的对象是topic。我们可以为每类数据创建一个topic，把向topic发布消息的客户端称作producer，从topic订阅消息的客户端称作consumer。Producers和consumers可以同时从多个topic读写数据。一个kafka集群由一个或多个broker服务器组成，它负责持久化和备份具体的kafka消息。</li>
<li>Broker：Kafka节点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li>
<li>Topic：一类消息，消息存放的目录即主题，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。</li>
<li>Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列</li>
<li>Segment：partition物理上由多个segment组成，每个Segment存着message信息</li>
<li>Producer : 生产message发送到topic</li>
<li>Consumer : 订阅topic消费message, consumer作为一个线程来消费</li>
<li>Consumer Group：一个Consumer Group包含多个consumer, 这个是预先在配置文件中配置好的。各个consumer（consumer 线程）可以组成一个组（Consumer group ），partition中的每个message只能被组（Consumer group ） 中的一个consumer（consumer 线程 ）消费，如果一个message可以被多个consumer（consumer 线程 ） 消费的话，那么这些consumer必须在不同的组。Kafka不支持一个partition中的message由两个或两个以上的consumer thread来处理，即便是来自不同的consumer group的也不行。它不能像AMQ那样可以多个BET作为consumer去处理message，这是因为多个BET去消费一个Queue中的数据的时候，由于要保证不能多个线程拿同一条message，所以就需要行级别悲观所（for update）,这就导致了consume的性能下降，吞吐量不够。而kafka为了保证吞吐量，只允许一个consumer线程去访问一个partition。如果觉得效率不高的时候，可以加partition的数量来横向扩展，那么再加新的consumer thread去消费。这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。</li>
</ul>
<h2 id="kafka一些原理概念"><a href="#kafka一些原理概念" class="headerlink" title="kafka一些原理概念"></a>kafka一些原理概念</h2><p>1.持久化<br>kafka使用文件存储消息(append only log),这就直接决定kafka在性能上严重依赖文件系统的本身特性.且无论任何OS下,对文件系统本身的优化是非常艰难的.文件缓存/直接内存映射等是常用的手段.因为kafka是对日志文件进行append操作,因此磁盘检索的开支是较小的;同时为了减少磁盘写入的次数,broker会将消息暂时buffer起来,当消息的个数(或尺寸)达到一定阀值时,再flush到磁盘,这样减少了磁盘IO调用的次数.对于kafka而言,较高性能的磁盘,将会带来更加直接的性能提升.</p>
<p>2.性能<br>除磁盘IO之外,我们还需要考虑网络IO,这直接关系到kafka的吞吐量问题.kafka并没有提供太多高超的技巧;对于producer端,可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker;对于consumer端也是一样,批量fetch多条消息.不过消息量的大小可以通过配置文件来指定.对于kafka broker端,似乎有个sendfile系统调用可以潜在的提升网络IO的性能:将文件的数据映射到系统内存中,socket直接读取相应的内存区域即可,而无需进程再次copy和交换(这里涉及到”磁盘IO数据”/“内核内存”/“进程内存”/“网络缓冲区”,多者之间的数据copy).<br>其实对于producer/consumer/broker三者而言,CPU的开支应该都不大,因此启用消息压缩机制是一个良好的策略;压缩需要消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑.可以将任何在网络上传输的消息都经过压缩.kafka支持gzip/snappy等多种压缩方式.</p>
<p>3.负载均衡<br>kafka集群中的任何一个broker,都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”/“partitions leader列表”等信息(请参看zookeeper中的节点信息). 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接;消息由producer直接通过socket发送到broker,中间不会经过任何”路由层”.<br>异步发送，将多条消息暂且在客户端buffer起来,并将他们批量发送到broker;小数据IO太多,会拖慢整体的网络延迟,批量延迟发送事实上提升了网络效率;不过这也有一定的隐患,比如当producer失效时,那些尚未发送的消息将会丢失。</p>
<p>4.Topic模型<br>其他JMS实现,消息消费的位置是有prodiver保留,以便避免重复发送消息或者将没有消费成功的消息重发等,同时还要控制消息的状态.这就要求JMS broker需要太多额外的工作.在kafka中,partition中的消息只有一个consumer在消费,且不存在消息状态的控制,也没有复杂的消息确认机制,可见kafka broker端是相当轻量级的.当消息被consumer接收之后,consumer可以在本地保存最后消息的offset,并间歇性的向zookeeper注册offset.由此可见,consumer客户端也很轻量级。<br>kafka中consumer负责维护消息的消费记录,而broker则不关心这些,这种设计不仅提高了consumer端的灵活性,也适度的减轻了broker端设计的复杂度;这是和众多JMS prodiver的区别.此外,kafka中消息ACK的设计也和JMS有很大不同,kafka中的消息是批量(通常以消息的条数或者chunk的尺寸为单位)发送给consumer,当消息消费成功后,向zookeeper提交消息的offset,而不会向broker交付ACK.或许你已经意识到,这种”宽松”的设计,将会有”丢失”消息/“消息重发”的危险.</p>
<p>5.消息传输一致<br>Kafka提供3种消息传输一致性语义：最多1次，最少1次，恰好1次。<br>最少1次：可能会重传数据，有可能出现数据被重复处理的情况;<br>最多1次：可能会出现数据丢失情况;<br>恰好1次：并不是指真正只传输1次，只不过有一个机制。确保不会出现“数据被重复处理”和“数据丢失”的情况。</p>
<p>at most once: 消费者fetch消息,然后保存offset,然后处理消息;当client保存offset之后,但是在消息处理过程中consumer进程失效(crash),导致部分消息未能继续处理.那么此后可能其他consumer会接管,但是因为offset已经提前保存,那么新的consumer将不能fetch到offset之前的消息(尽管它们尚没有被处理),这就是”at most once”.<br>at least once: 消费者fetch消息,然后处理消息,然后保存offset.如果消息处理成功之后,但是在保存offset阶段zookeeper异常或者consumer失效,导致保存offset操作未能执行成功,这就导致接下来再次fetch时可能获得上次已经处理过的消息,这就是”at least once”.<br>“Kafka Cluster”到消费者的场景中可以采取以下方案来得到“恰好1次”的一致性语义：<br>最少1次＋消费者的输出中额外增加已处理消息最大编号：由于已处理消息最大编号的存在，不会出现重复处理消息的情况。</p>
<p>6.副本<br>kafka中,replication策略是基于partition,而不是topic;kafka将每个partition数据复制到多个server上,任何一个partition有一个leader和多个follower(可以没有);备份的个数可以通过broker配置文件来设定。leader处理所有的read-write请求,follower需要和leader保持同步.Follower就像一个”consumer”,消费消息并保存在本地日志中;leader负责跟踪所有的follower状态,如果follower”落后”太多或者失效,leader将会把它从replicas同步列表中删除.当所有的follower都将一条消息保存成功,此消息才被认为是”committed”,那么此时consumer才能消费它,这种同步策略,就要求follower和leader之间必须具有良好的网络环境.即使只有一个replicas实例存活,仍然可以保证消息的正常发送和接收,只要zookeeper集群存活即可.<br>选择follower时需要兼顾一个问题,就是新leader server上所已经承载的partition leader的个数,如果一个server上有过多的partition leader,意味着此server将承受着更多的IO压力.在选举新leader,需要考虑到”负载均衡”,partition leader较少的broker将会更有可能成为新的leader.</p>
<p>7.log<br>每个log entry格式为”4个字节的数字N表示消息的长度” + “N个字节的消息内容”;每个日志都有一个offset来唯一的标记一条消息,offset的值为8个字节的数字,表示此消息在此partition中所处的起始位置..每个partition在物理存储层面,有多个log file组成(称为segment).segment file的命名为”最小offset”.kafka.例如”00000000000.kafka”;其中”最小offset”表示此segment中起始消息的offset.<br>获取消息时,需要指定offset和最大chunk尺寸,offset用来表示消息的起始位置,chunk size用来表示最大获取消息的总长度(间接的表示消息的条数).根据offset,可以找到此消息所在segment文件,然后根据segment的最小offset取差值,得到它在file中的相对位置,直接读取输出即可.<br><img src="/images/kafka-2-4.png" alt=""></p>
<p>8.分布式<br>kafka使用zookeeper来存储一些meta信息,并使用了zookeeper watch机制来发现meta信息的变更并作出相应的动作(比如consumer失效,触发负载均衡等)<br>Broker node registry: 当一个kafka broker启动后,首先会向zookeeper注册自己的节点信息(临时znode),同时当broker和zookeeper断开连接时,此znode也会被删除.<br>Broker Topic Registry: 当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息,仍然是一个临时znode.<br>Consumer and Consumer group: 每个consumer客户端被创建时,会向zookeeper注册自己的信息;此作用主要是为了”负载均衡”.一个group中的多个consumer可以交错的消费一个topic的所有partitions;简而言之,保证此topic的所有partitions都能被此group所消费,且消费时为了性能考虑,让partition相对均衡的分散到每个consumer上.<br>Consumer id Registry: 每个consumer都有一个唯一的ID(host:uuid,可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.<br>Consumer offset Tracking: 用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当group中一个消费者失效,其他consumer可以继续消费.<br>Partition Owner registry: 用来标记partition正在被哪个consumer消费.临时znode。此节点表达了”一个partition”只能被group下一个consumer消费,同时当group下某个consumer失效,那么将会触发负载均衡(即:让partitions在多个consumer间均衡消费,接管那些”游离”的partitions)<br>当consumer启动时,所触发的操作:<br>A) 首先进行”Consumer id Registry”;<br>B) 然后在”Consumer id Registry”节点下注册一个watch用来监听当前group中其他consumer的”leave”和”join”;只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions).<br>C) 在”Broker id registry”节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance.</p>
<p>总结:<br>1) Producer端使用zookeeper用来”发现”broker列表,以及和Topic下每个partition leader建立socket连接并发送消息.<br>2) Broker端使用zookeeper用来注册broker信息,已经监测partition leader存活性.<br>3) Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息。</p>
<p>9.Leader的选择<br>Kafka的核心是日志文件，日志文件在集群中的同步是分布式数据系统最基础的要素。<br>如果leaders永远不会down的话我们就不需要followers了！一旦leader down掉了，需要在followers中选择一个新的leader.但是followers本身有可能延时太久或者crash，所以必须选择高质量的follower作为leader.必须保证，一旦一个消息被提交了，但是leader down掉了，新选出的leader必须可以提供这条消息。大部分的分布式系统采用了多数投票法则选择新的leader,对于多数投票法则，就是根据所有副本节点的状况动态的选择最适合的作为leader.Kafka并不是使用这种方法。<br>Kafka动态维护了一个同步状态的副本的集合（a set of in-sync replicas），简称ISR，在这个集合中的节点都是和leader保持高度一致的，任何一条消息必须被这个集合中的每个节点读取并追加到日志中了，才回通知外部这个消息已经被提交了。因此这个集合中的任何一个节点随时都可以被选为leader.ISR在ZooKeeper中维护。ISR中有f+1个节点，就可以允许在f个节点down掉的情况下不会丢失消息并正常提供服。ISR的成员是动态的，如果一个节点被淘汰了，当它重新达到“同步中”的状态时，他可以重新加入ISR.这种leader的选择方式是非常快速的，适合kafka的应用场景。<br>一个邪恶的想法：如果所有节点都down掉了怎么办？Kafka对于数据不会丢失的保证，是基于至少一个节点是存活的，一旦所有节点都down了，这个就不能保证了。<br>实际应用中，当所有的副本都down掉时，必须及时作出反应。可以有以下两种选择:</p>
<ol>
<li>等待ISR中的任何一个节点恢复并担任leader。</li>
<li>选择所有节点中（不只是ISR）第一个恢复的节点作为leader.<br>这是一个在可用性和连续性之间的权衡。如果等待ISR中的节点恢复，一旦ISR中的节点起不起来或者数据都是了，那集群就永远恢复不了了。如果等待ISR意外的节点恢复，这个节点的数据就会被作为线上数据，有可能和真实的数据有所出入，因为有些数据它可能还没同步到。Kafka目前选择了第二种策略，在未来的版本中将使这个策略的选择可配置，可以根据场景灵活的选择。<br>这种窘境不只Kafka会遇到，几乎所有的分布式数据系统都会遇到。</li>
</ol>
<p>10.副本管理<br>以上仅仅以一个topic一个分区为例子进行了讨论，但实际上一个Kafka将会管理成千上万的topic分区.Kafka尽量的使所有分区均匀的分布到集群所有的节点上而不是集中在某些节点上，另外主从关系也尽量均衡这样每个几点都会担任一定比例的分区的leader.<br>优化leader的选择过程也是很重要的，它决定了系统发生故障时的空窗期有多久。Kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller.</p>
<p>11.Leader与副本同步<br>对于某个分区来说，保存正分区的”broker”为该分区的”leader”，保存备份分区的”broker”为该分区的”follower”。备份分区会完全复制正分区的消息，包括消息的编号等附加属性值。为了保持正分区和备份分区的内容一致，Kafka采取的方案是在保存备份分区的”broker”上开启一个消费者进程进行消费，从而使得正分区的内容与备份分区的内容保持一致。一般情况下，一个分区有一个“正分区”和零到多个“备份分区”。可以配置“正分区+备份分区”的总数量，关于这个配置，不同主题可以有不同的配置值。注意，生产者，消费者只与保存正分区的”leader”进行通信。</p>
<p>Kafka允许topic的分区拥有若干副本，这个数量是可以配置的，你可以为每个topic配置副本的数量。Kafka会自动在每个副本上备份数据，所以当一个节点down掉时数据依然是可用的。<br>Kafka的副本功能不是必须的，你可以配置只有一个副本，这样其实就相当于只有一份数据。<br>创建副本的单位是topic的分区，每个分区都有一个leader和零或多个followers.所有的读写操作都由leader处理，一般分区的数量都比broker的数量多的多，各分区的leader均匀的分布在brokers中。所有的followers都复制leader的日志，日志中的消息和顺序都和leader中的一致。followers向普通的consumer那样从leader那里拉取消息并保存在自己的日志文件中。<br>许多分布式的消息系统自动的处理失败的请求，它们对一个节点是否着（alive）”有着清晰的定义。Kafka判断一个节点是否活着有两个条件：</p>
<ol>
<li>节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接。</li>
<li>如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久。<br>符合以上条件的节点准确的说应该是“同步中的（in sync）”，而不是模糊的说是“活着的”或是“失败的”。Leader会追踪所有“同步中”的节点，一旦一个down掉了，或是卡住了，或是延时太久，leader就会把它移除。至于延时多久算是“太久”，是由参数replica.lag.max.messages决定的，怎样算是卡住了，怎是由参数replica.lag.time.max.ms决定的。<br>只有当消息被所有的副本加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。Producer也可以选择是否等待消息被提交的通知，这个是由参数acks决定的。<br>Kafka保证只要有一个“同步中”的节点，“committed”的消息就不会丢失。</li>
</ol>
<p><img src="/images/kafka-2-5.png" alt=""></p>
<h2 id="kafka拓扑结构"><a href="#kafka拓扑结构" class="headerlink" title="kafka拓扑结构"></a>kafka拓扑结构</h2><p>一个典型的Kafka集群中包含若干Producer（可以是web前端FET，或者是服务器日志等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干ConsumerGroup，以及一个Zookeeper集群。Kafka通过Zookeeper管理Kafka集群配置：选举Kafka broker的leader，以及在Consumer Group发生变化时进行rebalance，因为consumer消费kafka topic的partition的offsite信息是存在Zookeeper的。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p>
<p>分析过程分为以下4个步骤：</p>
<p>topic中partition存储分布<br>partiton中文件存储方式 (partition在linux服务器上就是一个目录（文件夹）)<br>partiton中segment文件存储结构<br>在partition中如何通过offset查找message<br>通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。</p>
<h2 id="topic中partition存储分布"><a href="#topic中partition存储分布" class="headerlink" title="topic中partition存储分布"></a>topic中partition存储分布</h2><p>假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名 称分别为report_push、launch_info, partitions数量都为partitions=4</p>
<p>存储路径和目录规则为：</p>
<p>xxx/message-folder</p>
<p>  |–report_push-0<br>  |–report_push-1<br>  |–report_push-2<br>  |–report_push-3<br>  |–launch_info-0<br>  |–launch_info-1<br>  |–launch_info-2<br>  |–launch_info-3</p>
<p>在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。<br>消息发送时都被发送到一个topic，其本质就是一个目录，而topic由是由一些Partition组成,其组织结构如下图所示：</p>
<p>我们可以看到，Partition是一个Queue的结构，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition上，其中的每一个消息都被赋予了一个唯一的offset值。</p>
<p>Kafka集群会保存所有的消息，不管消息有没有被消费；我们可以设定消息的过期时间，只有过期的数据才会被自动清除以释放磁盘空间。比如我们设置消息过期时间为2天，那么这2天内的所有消息都会被保存到集群中，数据只有超过了两天才会被清除。</p>
<p>Kafka只维护在Partition中的offset值，因为这个offsite标识着这个partition的message消费到哪条了。Consumer每消费一个消息，offset就会加1。其实消息的状态完全是由Consumer控制的，Consumer可以跟踪和重设这个offset值，这样的话Consumer就可以读取任意位置的消息。</p>
<p>把消息日志以Partition的形式存放有多重考虑，第一，方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；第二就是可以提高并发，因为可以以Partition为单位读写了。</p>
<p>通过上面介绍的我们可以知道，kafka中的数据是持久化的并且能够容错的。Kafka允许用户为每个topic设置副本数量，副本数量决定了有几个broker来存放写入的数据。如果你的副本数量设置为3，那么一份数据就会被存放在3台不同的机器上，那么就允许有2个机器失败。一般推荐副本数量至少为2，这样就可以保证增减、重启机器时不会影响到数据消费。如果对数据持久化有更高的要求，可以把副本数量设置为3或者更多。</p>
<p>Kafka中的topic是以partition的形式存放的，每一个topic都可以设置它的partition数量，Partition的数量决定了组成topic的message的数量。Producer在生产数据时，会按照一定规则（这个规则是可以自定义的）把消息发布到topic的各个partition中。上面将的副本都是以partition为单位的，不过只有一个partition的副本会被选举成leader作为读写用。</p>
<p>关于如何设置partition值需要考虑的因素。一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition），因此，如果设置的partition的数量小于consumer的数量，就会有消费者消费不到数据。所以，推荐partition的数量一定要大于同时运行的consumer的数量。另外一方面，建议partition的数量大于集群broker的数量，这样leader partition就可以均匀的分布在各个broker中，最终使得集群负载均衡。在Cloudera,每个topic都有上百个partition。需要注意的是，kafka需要为每个partition分配一些内存来缓存消息数据，如果partition数量越大，就要为kafka分配更大的heap space。</p>
<h2 id="partiton中文件存储方式"><a href="#partiton中文件存储方式" class="headerlink" title="partiton中文件存储方式"></a>partiton中文件存储方式</h2><p>Kafka文件存储机制那些事<br>每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。<br>每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。<br>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。<br><img src="/images/kafka-2-6.png" alt=""></p>
<h2 id="partiton中segment文件存储结构"><a href="#partiton中segment文件存储结构" class="headerlink" title="partiton中segment文件存储结构"></a>partiton中segment文件存储结构</h2><p>producer发message到某个topic，message会被均匀的分布到多个partition上（随机或根据用户指定的回调函数进行分布），kafka broker收到message往对应partition的最后一个segment上添加该消息，当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘，只有flush到磁盘上的消息consumer才能消费，segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。</p>
<p>每个part在内存中对应一个index，记录每个segment中的第一条消息偏移。<br>segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.<br>segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个全局partion的最大offset(偏移message数)。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</p>
<p>每个segment中存储很多条消息，消息id由其逻辑位置决定，即从消息id可直接定位到消息的存储位置，避免id到位置的额外映射。<br>下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：<br><img src="/images/kafka-2-7.png" alt=""><br>Kafka文件存储机制那些事</p>
<p>以上述图2中一对segment file文件为例，说明segment中index&lt;—-&gt;data file对应关系物理结构如下：</p>
<p>Kafka文件存储机制那些事</p>
<p>上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。其中以索引文件中 元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移 地址为497。</p>
<p>从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：<br><img src="/images/kafka-2-8.png" alt=""><br>Kafka文件存储机制那些事</p>
<p>参数说明：<br><img src="/images/kafka-2-13.png" alt=""></p>
<h2 id="在partition中如何通过offset查找message"><a href="#在partition中如何通过offset查找message" class="headerlink" title="在partition中如何通过offset查找message"></a>在partition中如何通过offset查找message</h2><p>例如读取offset=368776的message，需要通过下面2个步骤查找。</p>
<p>第一步查找segment file</p>
<p>上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件 00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset<strong>二分查找</strong>文件列表，就可以快速定位到具体文件。</p>
<p>当offset=368776时定位到00000000000000368769.index|log</p>
<p>第二步通过segment file查找message通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和 00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到 offset=368776为止。</p>
<p>segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它 比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。</p>
<p>kafka会记录offset到zk中。但是，zk client api对zk的频繁写入是一个低效的操作。0.8.2 kafka引入了native offset storage，将offset管理从zk移出，并且可以做到水平扩展。其原理就是利用了kafka的compacted topic，offset以consumer group,topic与partion的组合作为key直接提交到compacted topic中。同时Kafka又在内存中维护了的三元组来维护最新的offset信息，consumer来取最新offset信息的时候直接内存里拿即可。当然，kafka允许你快速的checkpoint最新的offset信息到磁盘上。</p>
<h1 id="Partition-Replication原则"><a href="#Partition-Replication原则" class="headerlink" title="Partition Replication原则"></a>Partition Replication原则</h1><p>Kafka高效文件存储设计特点</p>
<p>Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。<br>通过索引信息可以快速定位message和确定response的最大大小。<br>通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。<br>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</p>
<h2 id="Kafka集群partition-replication默认自动分配分析"><a href="#Kafka集群partition-replication默认自动分配分析" class="headerlink" title="Kafka集群partition replication默认自动分配分析"></a>Kafka集群partition replication默认自动分配分析</h2><p>下面以一个Kafka集群中4个Broker举例，创建1个topic包含4个Partition，2 Replication；数据Producer流动如图所示：<br>(1)<br><img src="/images/kafka-2-9.png" alt=""></p>
<p>(2)当集群中新增2节点，Partition增加到6个时分布情况如下：<br><img src="/images/kafka-2-10.png" alt=""></p>
<p>副本分配逻辑规则如下：<br>在Kafka集群中，每个Broker都有均等分配Partition的Leader机会。<br>上述图Broker Partition中，箭头指向为副本，以Partition-0为例:broker1中parition-0为Leader，Broker2中Partition-0为副本。<br>上述图种每个Broker(按照BrokerId有序)依次分配主Partition,下一个Broker为副本，如此循环迭代分配，多副本都遵循此规则。</p>
<p>副本分配算法如下：<br>将所有N Broker和待分配的i个Partition排序.<br>将第i个Partition分配到第(i mod n)个Broker上.<br>将第i个Partition的第j个副本分配到第((i + j) mod n)个Broker上.</p>
<h1 id="Kafka-Broker一些特性"><a href="#Kafka-Broker一些特性" class="headerlink" title="Kafka Broker一些特性"></a>Kafka Broker一些特性</h1><h2 id="无状态的Kafka-Broker"><a href="#无状态的Kafka-Broker" class="headerlink" title="无状态的Kafka Broker :"></a>无状态的Kafka Broker :</h2><ol>
<li>Broker没有副本机制，一旦broker宕机，该broker的消息将都不可用。</li>
<li>Broker不保存订阅者的状态，由订阅者自己保存。</li>
<li>无状态导致消息的删除成为难题（可能删除的消息正在被订阅），kafka采用基于时间的SLA(服务水平保证)，消息保存一定时间（通常为7天）后会被删除。</li>
<li>消息订阅者可以rewind back到任意位置重新进行消费，当订阅者故障时，可以选择最小的offset进行重新读取消费消息。</li>
</ol>
<h2 id="message的交付与生命周期-："><a href="#message的交付与生命周期-：" class="headerlink" title="message的交付与生命周期 ："></a>message的交付与生命周期 ：</h2><ol>
<li>不是严格的JMS， 因此kafka对消息的重复、丢失、错误以及顺序型没有严格的要求。（这是与AMQ最大的区别）</li>
<li>kafka提供at-least-once delivery,即当consumer宕机后，有些消息可能会被重复delivery。</li>
<li>因每个partition只会被consumer group内的一个consumer消费，故kafka保证每个partition内的消息会被顺序的订阅。</li>
<li>Kafka为每条消息为每条消息计算CRC校验，用于错误检测，crc校验不通过的消息会直接被丢弃掉。</li>
</ol>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>Kafka支持以集合（batch）为单位发送消息，在此基础上，Kafka还支持对消息集合进行压缩，Producer端可以通过GZIP或Snappy格式对消息集合进行压缩。Producer端进行压缩之后，在Consumer端需进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力，在对大数据处理上，瓶颈往往体现在网络上而不是CPU。</p>
<p>那么如何区分消息是压缩的还是未压缩的呢，Kafka在消息头部添加了一个描述压缩属性字节，这个字节的后两位表示消息的压缩采用的编码，如果后两位为0，则表示消息未被压缩。</p>
<h2 id="消息可靠性"><a href="#消息可靠性" class="headerlink" title="消息可靠性"></a>消息可靠性</h2><p>在消息系统中，保证消息在生产和消费过程中的可靠性是十分重要的，在实际消息传递过程中，可能会出现如下三中情况：</p>
<ul>
<li><p>一个消息发送失败</p>
</li>
<li><p>一个消息被发送多次</p>
</li>
<li><p>最理想的情况：exactly-once ,一个消息发送成功且仅发送了一次</p>
</li>
</ul>
<p>有许多系统声称它们实现了exactly-once，但是它们其实忽略了生产者或消费者在生产和消费过程中有可能失败的情况。比如虽然一个Producer成功发送一个消息，但是消息在发送途中丢失，或者成功发送到broker，也被consumer成功取走，但是这个consumer在处理取过来的消息时失败了。</p>
<p>从Producer端看：Kafka是这么处理的，当一个消息被发送后，Producer会等待broker成功接收到消息的反馈（可通过参数控制等待时间），如果消息在途中丢失或是其中一个broker挂掉，Producer会重新发送（我们知道Kafka有备份机制，可以通过参数控制是否等待所有备份节点都收到消息）。</p>
<p>从Consumer端看：前面讲到过partition，broker端记录了partition中的一个offset值，这个值指向Consumer下一个即将消费message。当Consumer收到了消息，但却在处理过程中挂掉，此时Consumer可以通过这个offset值重新找到上一个消息再进行处理。Consumer还有权限控制这个offset值，对持久化到broker端的消息做任意处理。</p>
<h2 id="备份机制"><a href="#备份机制" class="headerlink" title="备份机制"></a>备份机制</h2><p>备份机制是Kafka0.8版本的新特性，备份机制的出现大大提高了Kafka集群的可靠性、稳定性。有了备份机制后，Kafka允许集群中的节点挂掉后而不影响整个集群工作。一个备份数量为n的集群允许n-1个节点失败。在所有备份节点中，有一个节点作为lead节点，这个节点保存了其它备份节点列表，并维持各个备份间的状体同步。下面这幅图解释了Kafka的备份机制:<br><img src="/images/kafka-2-11.png" alt=""></p>
<h2 id="Kafka高效性相关设计"><a href="#Kafka高效性相关设计" class="headerlink" title="Kafka高效性相关设计"></a>Kafka高效性相关设计</h2><p>4.6.1 消息的持久化<br>Kafka高度依赖文件系统来存储和缓存消息(AMQ的nessage是持久化到mysql数据库中的)，因为一般的人认为磁盘是缓慢的，这导致人们对持久化结构具有竞争性持怀疑态度。其实，磁盘的快或者慢，这决定于我们如何使用磁盘。因为磁盘线性写的速度远远大于随机写。线性读写在大多数应用场景下是可以预测的。<br>4.6.2 常数时间性能保证<br>每个Topic的Partition的是一个大文件夹，里面有无数个小文件夹segment，但partition是一个队列，队列中的元素是segment,消费的时候先从第0个segment开始消费，新来message存在最后一个消息队列中。对于segment也是对队列，队列元素是message,有对应的offsite标识是哪个message。消费的时候先从这个segment的第一个message开始消费，新来的message存在segment的最后。</p>
<p>消息系统的持久化队列可以构建在对一个文件的读和追加上，就像一般情况下的日志解决方案。它有一个优点，所有的操作都是常数时间，并且读写之间不会相互阻塞。这种设计具有极大的性能优势：最终系统性能和数据大小完全无关，服务器可以充分利用廉价的硬盘来提供高效的消息服务。</p>
<p>事实上还有一点，磁盘空间的无限增大而不影响性能这点，意味着我们可以提供一般消息系统无法提供的特性。比如说，消息被消费后不是立马被删除，我们可以将这些消息保留一段相对比较长的时间（比如一个星期）。</p>
<h1 id="Kafka-生产者-消费者"><a href="#Kafka-生产者-消费者" class="headerlink" title="Kafka 生产者-消费者"></a>Kafka 生产者-消费者</h1><p>消息系统通常都会由生产者，消费者，Broker三大部分组成，生产者会将消息写入到Broker，消费者会从Broker中读取出消息，不同的MQ实现的Broker实现会有所不同，不过Broker的本质都是要负责将消息落地到服务端的存储系统中。具体步骤如下：</p>
<p>生产者客户端应用程序产生消息：<br>客户端连接对象将消息包装到请求中发送到服务端<br>服务端的入口也有一个连接对象负责接收请求，并将消息以文件的形式存储起来<br>服务端返回响应结果给生产者客户端</p>
<p>消费者客户端应用程序消费消息：<br>客户端连接对象将消费信息也包装到请求中发送给服务端<br>服务端从文件存储系统中取出消息<br>服务端返回响应结果给消费者客户端<br>客户端将响应结果还原成消息并开始处理消息</p>
<h2 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h2><p>Producers直接发送消息到broker上的leader partition，不需要经过任何中介或其他路由转发。为了实现这个特性，kafka集群中的每个broker都可以响应producer的请求，并返回topic的一些元信息，这些元信息包括哪些机器是存活的，topic的leader partition都在哪，现阶段哪些leader partition是可以直接被访问的。</p>
<p>Producer客户端自己控制着消息被推送到哪些partition。实现的方式可以是随机分配、实现一类随机负载均衡算法，或者指定一些分区算法。Kafka提供了接口供用户实现自定义的partition，用户可以为每个消息指定一个partitionKey，通过这个key来实现一些hash分区算法。比如，把userid作为partitionkey的话，相同userid的消息将会被推送到同一个partition。</p>
<p>以Batch的方式推送数据可以极大的提高处理效率，kafka Producer 可以将消息在内存中累计到一定数量后作为一个batch发送请求。Batch的数量大小可以通过Producer的参数控制，参数值可以设置为累计的消息的数量（如500条）、累计的时间间隔（如100ms）或者累计的数据大小(64KB)。通过增加batch的大小，可以减少网络请求和磁盘IO的次数，当然具体参数设置需要在效率和时效性方面做一个权衡。</p>
<p>Producers可以异步的并行的向kafka发送消息，但是通常producer在发送完消息之后会得到一个future响应，返回的是offset值或者发送过程中遇到的错误。这其中有个非常重要的参数“acks”,这个参数决定了producer要求leader partition 收到确认的副本个数，如果acks设置数量为0，表示producer不会等待broker的响应，所以，producer无法知道消息是否发送成功，这样有可能会导致数据丢失，但同时，acks值为0会得到最大的系统吞吐量。</p>
<p>若acks设置为1，表示producer会在leader partition收到消息时得到broker的一个确认，这样会有更好的可靠性，因为客户端会等待直到broker确认收到消息。若设置为-1，producer会在所有备份的partition收到消息时得到broker的确认，这个设置可以得到最高的可靠性保证。</p>
<p>Kafka 消息有一个定长的header和变长的字节数组组成。因为kafka消息支持字节数组，也就使得kafka可以支持任何用户自定义的序列号格式或者其它已有的格式如Apache Avro、protobuf等。Kafka没有限定单个消息的大小，但我们推荐消息大小不要超过1MB,通常一般消息大小都在1~10kB之前。</p>
<p>发布消息时，kafka client先构造一条消息，将消息加入到消息集set中（kafka支持批量发布，可以往消息集合中添加多条消息，一次行发布），send消息时，producer client需指定消息所属的topic。</p>
<h2 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h2><p>Kafka提供了两套consumer api，分为high-level api和sample-api。Sample-api 是一个底层的API，它维持了一个和单一broker的连接，并且这个API是完全无状态的，每次请求都需要指定offset值，因此，这套API也是最灵活的。</p>
<p>在kafka中，当前读到哪条消息的offset值是由consumer来维护的，因此，consumer可以自己决定如何读取kafka中的数据。比如，consumer可以通过重设offset值来重新消费已消费过的数据。不管有没有被消费，kafka会保存数据一段时间，这个时间周期是可配置的，只有到了过期时间，kafka才会删除这些数据。（这一点与AMQ不一样，AMQ的message一般来说都是持久化到mysql中的，消费完的message会被delete掉）</p>
<p>High-level API封装了对集群中一系列broker的访问，可以透明的消费一个topic。它自己维持了已消费消息的状态，即每次消费的都是下一个消息。</p>
<p>High-level API还支持以组的形式消费topic，如果consumers有同一个组名，那么kafka就相当于一个队列消息服务，而各个consumer均衡的消费相应partition中的数据。若consumers有不同的组名，那么此时kafka就相当与一个广播服务，会把topic中的所有消息广播到每个consumer。</p>
<p>High level api和Low level api是针对consumer而言的，和producer无关。</p>
<p>High level api是consumer读的partition的offsite是存在zookeeper上。High level api 会启动另外一个线程去每隔一段时间，offsite自动同步到zookeeper上。换句话说，如果使用了High level api， 每个message只能被读一次，一旦读了这条message之后，无论我consumer的处理是否ok。High level api的另外一个线程会自动的把offiste+1同步到zookeeper上。如果consumer读取数据出了问题，offsite也会在zookeeper上同步。因此，如果consumer处理失败了，会继续执行下一条。这往往是不对的行为。因此，Best Practice是一旦consumer处理失败，直接让整个conusmer group抛Exception终止，但是最后读的这一条数据是丢失了，因为在zookeeper里面的offsite已经+1了。等再次启动conusmer group的时候，已经从下一条开始读取处理了。</p>
<p>Low level api是consumer读的partition的offsite在consumer自己的程序中维护。不会同步到zookeeper上。但是为了kafka manager能够方便的监控，一般也会手动的同步到zookeeper上。这样的好处是一旦读取某个message的consumer失败了，这条message的offsite我们自己维护，我们不会+1。下次再启动的时候，还会从这个offsite开始读。这样可以做到exactly once对于数据的准确性有保证。</p>
<p>对于Consumer group：</p>
<ol>
<li>允许consumer group（包含多个consumer，如一个集群同时消费）对一个topic进行消费，不同的consumer group之间独立消费。</li>
<li>为了对减小一个consumer group中不同consumer之间的分布式协调开销，指定partition为最小的并行消费单位，即一个group内的consumer只能消费不同的partition。<br><img src="/images/kafka-2-12.png" alt=""></li>
</ol>
<p>Consumer与Partition的关系：</p>
<ul>
<li>如果consumer比partition多，是浪费，因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数</li>
<li>如果consumer比partition少，一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀</li>
<li>如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同</li>
<li>增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化</li>
<li>High-level接口中获取不到数据的时候是会block的</li>
</ul>
<p>负载低的情况下可以每个线程消费多个partition。但负载高的情况下，Consumer 线程数最好和Partition数量保持一致。如果还是消费不过来，应该再开 Consumer 进程，进程内线程数同样和分区数一致。</p>
<p>消费消息时，kafka client需指定topic以及partition number（每个partition对应一个逻辑日志流，如topic代表某个产品线，partition代表产品线的日志按天切分的结果），consumer client订阅后，就可迭代读取消息，如果没有消息，consumer client会阻塞直到有新的消息发布。consumer可以累积确认接收到的消息，当其确认了某个offset的消息，意味着之前的消息也都已成功接收到，此时broker会更新zookeeper上地offset registry。</p>
<h2 id="高效的数据传输"><a href="#高效的数据传输" class="headerlink" title="高效的数据传输"></a>高效的数据传输</h2><ol>
<li>发布者每次可发布多条消息（将消息加到一个消息集合中发布）， consumer每次迭代消费一条消息。</li>
<li>不创建单独的cache，使用系统的page cache。发布者顺序发布，订阅者通常比发布者滞后一点点，直接使用Linux的page cache效果也比较后，同时减少了cache管理及垃圾收集的开销。</li>
<li>使用sendfile优化网络传输，减少一次内存拷贝。</li>
</ol>
<h1 id="Kafka-与-Zookeeper"><a href="#Kafka-与-Zookeeper" class="headerlink" title="Kafka 与 Zookeeper"></a>Kafka 与 Zookeeper</h1><h2 id="Zookeeper-协调控制"><a href="#Zookeeper-协调控制" class="headerlink" title="Zookeeper 协调控制"></a>Zookeeper 协调控制</h2><ol>
<li>管理broker与consumer的动态加入与离开。(Producer不需要管理，随便一台计算机都可以作为Producer向Kakfa Broker发消息)</li>
<li>触发负载均衡，当broker或consumer加入或离开时会触发负载均衡算法，使得一<br>个consumer group内的多个consumer的消费负载平衡。（因为一个comsumer消费一个或多个partition，一个partition只能被一个consumer消费）</li>
<li>维护消费关系及每个partition的消费信息。</li>
</ol>
<h2 id="Zookeeper上的细节"><a href="#Zookeeper上的细节" class="headerlink" title="Zookeeper上的细节"></a>Zookeeper上的细节</h2><ol>
<li>每个broker启动后会在zookeeper上注册一个临时的broker registry，包含broker的ip地址和端口号，所存储的topics和partitions信息。</li>
<li>每个consumer启动后会在zookeeper上注册一个临时的consumer registry：包含consumer所属的consumer group以及订阅的topics。</li>
<li>每个consumer group关联一个临时的owner registry和一个持久的offset registry。对于被订阅的每个partition包含一个owner registry，内容为订阅这个partition的consumer id；同时包含一个offset registry，内容为上一次订阅的offset。</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/24/zookeeper-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/24/zookeeper-1/" itemprop="url">
                  Zookeeper全解析——Paxos作为灵魂
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-24 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-24T00:00:00+08:00">2017-10-24</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>非常努力想找到原文出处，不过在网上发现好多，但都是转载的，只能标注一下自己看到的博客地址：<a href="http://blog.sina.com.cn/s/blog_5d97745a0101ey63.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_5d97745a0101ey63.html</a></p>
<p>原计划在介绍完ZK Client之后就着手ZK Server的介绍，但是发现ZK Server所包含的内容实在太多，并不是简简单单一篇Blog就能搞定的。于是决定从基础搞起比较好。</p>
<p>那么ZK Server最基础的东西是什么呢？我想应该是Paxos了。所以本文会介绍Paxos以及它在ZK Server中对应的实现。</p>
<p>先说Paxos，它是一个基于消息传递的一致性算法，Leslie Lamport在1990年提出，近几年被广泛应用于分布式计算中，Google的Chubby，Apache的Zookeeper都是基于它的理论来实现的，Paxos还被认为是到目前为止唯一的分布式一致性算法，其它的算法都是Paxos的改进或简化。有个问题要提一下，Paxos有一个前提：没有拜占庭将军问题。就是说Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。</p>
<p>关于Paxos的具体描述可以在Wiki中找到：<a href="http://zh.wikipedia.org/zh-cn/Paxos算法。网上关于Paxos分析的文章也很多。这里希望用最简单的方式加以描述并建立起Paxos和ZK" target="_blank" rel="noopener">http://zh.wikipedia.org/zh-cn/Paxos算法。网上关于Paxos分析的文章也很多。这里希望用最简单的方式加以描述并建立起Paxos和ZK</a><br> Server的对应关系。</p>
<p>Paxos描述了这样一个场景，有一个叫做Paxos的小岛(Island)上面住了一批居民，岛上面所有的事情由一些特殊的人决定，他们叫做议员(Senator)。议员的总数(Senator Count)是确定的，不能更改。岛上每次环境事务的变更都需要通过一个提议(Proposal)，每个提议都有一个编号(PID)，这个编号是一直增长的，不能倒退。每个提议都需要超过半数((Senator<br> Count)/2 +1)的议员同意才能生效。每个议员只会同意大于当前编号的提议，包括已生效的和未生效的。如果议员收到小于等于当前编号的提议，他会拒绝，并告知对方：你的提议已经有人提过了。这里的当前编号是每个议员在自己记事本上面记录的编号，他不断更新这个编号。整个议会不能保证所有议员记事本上的编号总是相同的。现在议会有一个目标：保证所有的议员对于提议都能达成一致的看法。</p>
<p>好，现在议会开始运作，所有议员一开始记事本上面记录的编号都是0。有一个议员发了一个提议：将电费设定为1元/度。他首先看了一下记事本，嗯，当前提议编号是0，那么我的这个提议的编号就是1，于是他给所有议员发消息：1号提议，设定电费1元/度。其他议员收到消息以后查了一下记事本，哦，当前提议编号是0，这个提议可接受，于是他记录下这个提议并回复：我接受你的1号提议，同时他在记事本上记录：当前提议编号为1。发起提议的议员收到了超过半数的回复，立即给所有人发通知：1号提议生效！收到的议员会修改他的记事本，将1好提议由记录改成正式的法令，当有人问他电费为多少时，他会查看法令并告诉对方：1元/度。</p>
<p>现在看冲突的解决：假设总共有三个议员S1-S3，S1和S2同时发起了一个提议:1号提议，设定电费。S1想设为1元/度, S2想设为2元/度。结果S3先收到了S1的提议，于是他做了和前面同样的操作。紧接着他又收到了S2的提议，结果他一查记事本，咦，这个提议的编号小于等于我的当前编号1，于是他拒绝了这个提议：对不起，这个提议先前提过了。于是S2的提议被拒绝，S1正式发布了提议:<br> 1号提议生效。S2向S1或者S3打听并更新了1号法令的内容，然后他可以选择继续发起2号提议。</p>
<p>好，我觉得Paxos的精华就这么多内容。现在让我们来对号入座，看看在ZK Server里面Paxos是如何得以贯彻实施的。</p>
<p>小岛(Island)——ZK Server Cluster</p>
<p>议员(Senator)——ZK Server</p>
<p>提议(Proposal)——ZNode Change(Create/Delete/SetData…)</p>
<p>提议编号(PID)——Zxid(ZooKeeper Transaction Id)</p>
<p>正式法令——所有ZNode及其数据</p>
<p>貌似关键的概念都能一一对应上，但是等一下，Paxos岛上的议员应该是人人平等的吧，而ZK Server好像有一个Leader的概念。没错，其实Leader的概念也应该属于Paxos范畴的。如果议员人人平等，在某种情况下会由于提议的冲突而产生一个“活锁”（所谓活锁我的理解是大家都没有死，都在动，但是一直解决不了冲突问题）。Paxos的作者Lamport在他的文章”The<br> Part-Time Parliament“中阐述了这个问题并给出了解决方案——在所有议员中设立一个总统，只有总统有权发出提议，如果议员有自己的提议，必须发给总统并由总统来提出。好，我们又多了一个角色：总统。</p>
<p>总统——ZK Server Leader</p>
<p>又一个问题产生了，总统怎么选出来的？oh, my god! It’s a long story. 在淘宝核心系统团队的Blog上面有一篇文章是介绍如何选出总统的，有兴趣的可以去看看：<a href="http://rdc.taobao.com/blog/cs/?p=162" target="_blank" rel="noopener">http://rdc.taobao.com/blog/cs/?p=162</a></p>
<p>现在我们假设总统已经选好了，下面看看ZK Server是怎么实施的。</p>
<p>情况一：</p>
<p>屁民甲(Client)到某个议员(ZK Server)那里询问(Get)某条法令的情况(ZNode的数据)，议员毫不犹豫的拿出他的记事本(local storage)，查阅法令并告诉他结果，同时声明：我的数据不一定是最新的。你想要最新的数据？没问题，等着，等我找总统Sync一下再告诉你。</p>
<p>情况二：</p>
<p>屁民乙(Client)到某个议员(ZK Server)那里要求政府归还欠他的一万元钱，议员让他在办公室等着，自己将问题反映给了总统，总统询问所有议员的意见，多数议员表示欠屁民的钱一定要还，于是总统发表声明，从国库中拿出一万元还债，国库总资产由100万变成99万。屁民乙拿到钱回去了(Client函数返回)。</p>
<p>情况三：</p>
<p>总统突然挂了，议员接二连三的发现联系不上总统，于是各自发表声明，推选新的总统，总统大选期间政府停业，拒绝屁民的请求。</p>
<p>呵呵，到此为止吧，当然还有很多其他的情况，但这些情况总是能在Paxos的算法中找到原型并加以解决。这也正是我们认为Paxos是Zookeeper的灵魂的原因。当然ZK Server还有很多属于自己特性的东西：Session, Watcher，Version等等等等，需要我们花更多的时间去研究和学习。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/17/scala-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/17/scala-1/" itemprop="url">
                  scala之隐式转换
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-17 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-17T00:00:00+08:00">2017-10-17</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/16/git-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/16/git-1/" itemprop="url">
                  更新Xcode导致Git报错的问题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-16T00:00:00+08:00">2017-10-16</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>更新Xcode以后使用Git报错:<br><img src="/images/git-1-1.png" alt=""></p>
<h1 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h1><p>1、输入<code>sudo xcodebuild -license</code>打开条款<br>2、将条款拖到最后，输入agree同意条款然后就可以正常使用了<br><img src="/images/git-1-2.png" alt=""></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.thinkyixia.com/2017/10/16/sparkstreaming-kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="那个谁">
      <meta itemprop="description" content="吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="静水流深">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/16/sparkstreaming-kafka/" itemprop="url">
                  spark streaming集成kafka的两种方式及区别
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-16T00:00:00+08:00">2017-10-16</time>
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/技术/" itemprop="url" rel="index"><span itemprop="name">技术</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Spark Streaming 与 Kafka 集成接收数据的方式有两种：</p>
<ul>
<li>Receiver-based Approach</li>
<li>Direct Approach (No Receivers)</li>
</ul>
<h1 id="两种方式的区别"><a href="#两种方式的区别" class="headerlink" title="两种方式的区别"></a>两种方式的区别</h1><h2 id="Receiver-based-Approach"><a href="#Receiver-based-Approach" class="headerlink" title="Receiver-based Approach"></a>Receiver-based Approach</h2><p>构造函数为：KafkaUtils.createDstream(ssc, [zk], [consumer group id], [per-topic,partitions] )<br>使用了receivers来接收数据，利用的是Kafka高层次的消费者api，对于所有的receivers接收到的数据将会保存在spark executors中，然后通过Spark Streaming启动job来处理这些数据，默认会丢失，可启用WAL日志（开启后效率会下降），该日志存储在HDFS上。</p>
<p>1、创建一个receiver来对kafka进行定时拉取数据，ssc的rdd分区和kafka的topic分区不是一个概念，故如果增加特定主体分区数仅仅是增加一个receiver中消费topic的线程数，并不增加spark的并行处理数据数量<br>2、对于不同的group和topic可以使用多个receivers创建不同的DStream<br>3、如果启用了WAL,需要设置存储级别,即<br><code>KafkaUtils.createStream(….,StorageLevel.MEMORY_AND_DISK_SER)</code></p>
<h2 id="Direct-Approach-No-Receivers"><a href="#Direct-Approach-No-Receivers" class="headerlink" title="Direct Approach (No Receivers)"></a>Direct Approach (No Receivers)</h2><p>区别Receiver接收数据，这种方式定期地从kafka的topic+partition中查询最新的偏移量，再根据偏移量范围在每个batch里面处理数据，使用的是kafka的简单消费者(SimpleConsumer)api</p>
<p>这种方式有如下优点：<br>1、简化并行读取：<br>如果要读取多个partition,不需要创建多个输入DStream然后对它们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。<br>2、高性能：<br>如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。<br>3、恰好恰好一次语义(Exactly-once-semantics):<br>基于receiver的方式，读取kafka数据是使用Kafka的高阶API来将消费过的offset保存在ZooKeeper中的。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理恰好一次，这种方式存在zookeeper和spark的偏移量不一致(同步)的情况，导致数据会被多次处理。<br>基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中，Spark自己一定是同步的，消除了Spark与zookeeper中偏移量不一致的问题，因此可以保证数据消费恰好一次。缺点是无法使用基于zookeeper的kafka监控工具。</p>
<h1 id="Direct方式代码实现逻辑"><a href="#Direct方式代码实现逻辑" class="headerlink" title="Direct方式代码实现逻辑"></a>Direct方式代码实现逻辑</h1><p>1、通过 <code>val kafkaCluster = new KafkaCluster(kafkaParams)</code> 创建一个kafkaCluster，kafkaCluster负责与kafka交互。<br>2、通过 <code>val kafkaPartitionsE = kafkaCluster.getPartition(topics)</code> 获取该topic集合对应的各个partition集合（一个topic-&gt;partition集合）<br>3、通过 <code>val kafkaPartitions = kafkaPartitionsE.right.get</code> 获取TopicAndPartition的集合<br>4、通过 <code>val consumerOffsetsE = kafkaCluster.getConsumerOffset(groupId,kafkaPartitions)</code> 获取到该groupId在该topic的每个partition中的offset；<br>5、通过 <code>kafkaCluster.getEarliestLeaderOffset(kafkaPartitions).regit.get</code> 或者<br><code>kafkaCluster.getLatestLeaderOffset(kafkaPartitions).regit.get</code>分别获取该topic的每个partition现在的最小、最大offset，对该groupId对应的各topic的各partition的offset是否在区间内进行判断。<br>然后返回一个(TopicAndPartition,offset)的map集合。<br>6、然后根据 <code>val kafkaStream = kafkaUtils.createDirectStream()</code> 方法创建DirectKafkaInputDStream；<br>创建时需要根据情况设置 <code>auto.offset.reset</code> 参数;<br>该处若设置成smallest，从最小的开始消费;若设置为largest，从最新的开始读取；不设置即从上次消费的地方读取;<br>7、数据取出来处理完之后，通过遍历kafkaStream，获取每个分区的offset，使用 <code>kafkaCluster.setConsumerOffsets</code> 方法同步到Zookeeper中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kafkaStream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      //得到已读取的topic每个分区的offset</span><br><span class="line">      val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (offsets &lt;- offsetsList) &#123;</span><br><span class="line">        val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)</span><br><span class="line"></span><br><span class="line">        // 更新offset到kafkaCluster ,将topic每个分区的offset保存到ZooKeeper中</span><br><span class="line">        val o = kafkaCluster.setConsumerOffsets(groupId, Map(topicAndPartition -&gt; offsets.untilOffset))</span><br><span class="line">        <span class="keyword">if</span> (o.isLeft) &#123;</span><br><span class="line">          println(s<span class="string">"Error updating the offset to Kafka cluster: <span class="variable">$&#123;o.left.get&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">那个谁</p>
              <p class="site-description motion-element" itemprop="description">吾资之昏，不逮人也，吾材之庸，不逮人也；旦旦而学之，久而不怠焉，迄乎成，而亦不知其昏与庸也。吾资之聪，倍人也，吾材之敏，倍人也；屏弃而不用，其与昏与庸无以异也。圣人之道，卒于鲁也传之。然则昏庸聪敏之用，岂有常哉？</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">46</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">26</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">那个谁</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  





  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
